---
title: "Lab8_Korb_Vignette"
author: "Kelsey Korb"
date: '`r Sys.Date()`'
output:
  rmarkdown::html_document:
    toc: true
    toc_float: true
vignette: >
  %\VignetteIndexEntry{Lab8_Korb_Vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---



```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(Lab8KORBMath4753)
```

# Nature of Package

The *Lab8KORBMath4753* package is designed to support conceptual and computational exploration of probabilistic transformations and distributional theory using simulation and visualization tools. Rooted in the context of exponential and gamma distributions, this package provides both analytical and empirical methods for studying random variables derived from transformations such as $A = \frac{X}{X + Y}$ and $B = X + Y$, where $X, Y \sim \text{Exp}(1)$.

The core functionality includes:

- Functions for deriving and plotting marginal densities and cumulative distributions.
- Simulation functions that approximate theoretical quantities such as means, variances, and quantiles.
- Interactive visualization through an integrated Shiny app for dynamic exploration of simulated distributions.
- Clean integration with R packaging standards, including documentation, roxygen2 support, and reproducibility via vignettes.

# Tasks

## Task 1: Theory

### Proof of Joint PDF

Let $X$ and $Y$ be independent random variables where:

- $X \sim \text{Exp}(1)$ with density $f_1(x) = e^{-x}, \quad x > 0$
- $Y \sim \text{Exp}(1)$ with density $f_2(y) = e^{-y}, \quad y > 0$

Show that the joint probability density function is:

$$
f(x, y) = e^{-(x + y)}, \quad x, y > 0
$$

**Solution**

Since $X$ and $Y$ are independent, their joint density is:

$$ 
\begin{align*}
f(x, y)                &= f_1(x) \cdot f_2(y) \\
f_1(x) \cdot f_2(y)    &= e^{-x} \cdot e^{-y} \\
e^{-x} \cdot e^{-y}    &= e^{-(x + y)}, \quad x, y > 0
\end{align*}
$$

This confirms that the joint PDF is the product of the marginals and simplifies as shown.

### Change of Variables

Let $X$ and $Y$ be independent random variables, each distributed as $\text{Exp}(1)$:

- $f_1(x) = e^{-x}, \quad x > 0$
- $f_2(y) = e^{-y}, \quad y > 0$

The transformation is defined as,

- $A = \frac{X}{X + Y}$
- $B = X + Y$

Find the joint density $g(a, b)$ of $(A, B)$:

$$
g(a, b) = b e^{-b}, \quad 0 < a < 1, \ b > 0
$$

**Solution**

We begin by expressing $X$ and $Y$ in terms of $a$ and $b$:

- $X = ab$
- $Y = (1 - a)b$

Then the original joint PDF becomes:

$$
f(x, y) = e^{-(x + y)} = e^{-b}
$$

We use the change of variables formula:

$$
g(a, b) = f(x(a, b), y(a, b)) \cdot \left| \det(J) \right|
$$

**Jacobian Determinant**

Let:

- $x = ab$
- $y = (1 - a)b$

Then the Jacobian matrix is:

$$
J = \begin{bmatrix}
\frac{\partial x}{\partial a} & \frac{\partial x}{\partial b} \\
\frac{\partial y}{\partial a} & \frac{\partial y}{\partial b}
\end{bmatrix}
=
\begin{bmatrix}
b & a \\
-b & 1 - a
\end{bmatrix}
$$

Computing the determinant:

$$
\det(J) = b(1 - a) + ab = b
$$

**Final Answer**

Thus the transformed density is:

$$
g(a, b) = e^{-b} \cdot b = b e^{-b}, \quad 0 < a < 1, \ b > 0
$$

### $g(a) = 1$

**Marginal Density of A**

To find the marginal density $g(a)$, integrate out $b$:

$$
g(a) = \int_0^\infty g(a, b)\, db = \int_0^\infty b e^{-b}\, db
$$

Using a standard integral:

$$
\int_0^\infty b e^{-b} \, db = \Gamma(2) = 1
$$

Hence:

$$
g(a) = 1, \quad \text{for } 0 < a < 1
$$

The marginal distribution of $A$ is uniform on $(0, 1)$.

### Distribution and Parameters of $A$

Let $X$ and $Y$ be independent exponential random variables with rate 1:

- $X \sim \text{Exp}(1)$
- $Y \sim \text{Exp}(1)$

Define:

$$
A = \frac{X}{X + Y}
$$

We aim to determine the distribution of $A$ and identify its parameters.

**Derivation**

We use the fact that if $X$ and $Y$ are independent and identically distributed $\text{Exp}(\lambda)$ variables, then:

$$
A = \frac{X}{X + Y} \sim \text{Beta}(1, 1)
$$

For $\lambda = 1$, $X$ and $Y$ follow standard exponential distributions. Therefore:

- The ratio $A$ lies strictly between 0 and 1
- The probability density function (PDF) of $A$ is:

$$
g(a) = 1, \quad \text{for } 0 < a < 1
$$

This is the uniform distribution on $(0,1)$, which is equivalent to:

$$
A \sim \text{Beta}(1, 1)
$$

**Solution**

The variable $A = \dfrac{X}{X + Y}$ follows a **Beta distribution** with parameters:

- Shape parameters: $\alpha = 1$, $\beta = 1$
- Support: $0 < a < 1$

### Marginal Density of $B$

**Problem Statement**

Given the joint density:

$$
g(a, b) = b e^{-b}, \quad 0 < a < 1,\ b > 0
$$

Find the marginal density of $B$ by integrating out $a$.

**Derivation**

To obtain the marginal density $g(b)$, we integrate $g(a, b)$ over the range of $a$:

$$
g(b) = \int_0^1 g(a, b) \, da = \int_0^1 b e^{-b} \, da
$$

Since $b e^{-b}$ is **constant** with respect to $a$, the integral simplifies to:

$$
g(b) = b e^{-b} \int_0^1 da = b e^{-b} \cdot (1 - 0) = b e^{-b}
$$

**Solution**

The marginal density of $B$ is:

$$
g(b) = b e^{-b}, \quad b > 0
$$

This corresponds to a Gamma distribution with shape $\alpha = 2$ and rate $\lambda = 1$, i.e., $B \sim \text{Gamma}(2, 1)$.

### Plot $g(a)$ and $g(b)$

Funciton is packaged in 'LAB8KORBMATH4753::plot_joint_marginals()'

```{r}
plots <- plot_joint_marginals()
print(plots$ga_plot)
print(plots$gb_plot)
```

*Graph of $g(a)$*

The above graph illustrates the marginal density $g(a)$, where $a = \frac{X}{X + Y}$. That graph displays a constant horizontal line across $a \in [0, 1]$, supporting the fact that:
$$
g(a) = 1, \quad a \in [0, 1]
$$
which implies $a \sim \text{Uniform}(0, 1)$.

*Graph of $g(b)$*

The image depicts the marginal density function $g(b)$, where $b = X + Y$ and $X, Y \sim \text{Exp}(1)$. The graph is a smooth, green curve defined over the domain $b \in [0, 10]$, with the vertical axis representing density values between $0$ and approximately $0.35$.

The shape of the curve corresponds to the probability density function of a Gamma distribution with shape parameter $\alpha = 2$ and rate $\lambda = 1$:
$$
g(b) = b e^{-b}, \quad b > 0
$$
The density begins at $0$, peaks near $b = 1.5$, and then asymptotically decays toward zero as $b \to \infty$. This confirms the theoretical behavior of the Gamma(2, 1) distribution.

*Key Differences*

- **Distribution Type:** $g(a)$ is flat and uniform, whereas $g(b)$ is positively skewed with a peak, typical of Gamma distributions.
- **Support Range:** $a$ is bounded within $[0, 1]$, while $b$ is defined over $(0, \infty)$.
- **Shape Characteristics:** The uniform density suggests equal likelihood for all values of $a$, whereas the gamma density suggests small values of $b$ are more likely than large ones.

*Implications*

The contrast between $g(a)$ and $g(b)$ highlights how transformations of exponential random variables result in fundamentally different distributions. This reinforces the power of analytical derivation and simulation techniques within the package, which allows users to visualize and verify the behavior of derived statistics across domains.


### Derivation of Expected Value

Let $X$ and $Y$ be independent exponential random variables with rate 1:

- $X \sim \text{Exp}(1)$
- $Y \sim \text{Exp}(1)$

Define:

$$
A = \frac{X}{X + Y}
$$

Previously, we showed that $A \sim \text{Uniform}(0, 1)$, meaning its density is:

$$
g(a) = 1, \quad 0 < a < 1
$$

We now derive $E(A)$ and $V(A)$ using the definition of expected value.

**Expected Value of $A$**

By definition:

$$
E(A) = \int_0^1 a \cdot g(a) \, da = \int_0^1 a \cdot 1 \, da = \int_0^1 a \, da
$$

Compute the integral:

$$
\int_0^1 a \, da = \left[\frac{a^2}{2}\right]_0^1 = \frac{1}{2}
$$

So:

$$
E(A) = \frac{1}{2}
$$

**Variance of $A$**

We use the formula:

$$
V(A) = E(A^2) - [E(A)]^2
$$

First, compute $E(A^2)$:

$$
E(A^2) = \int_0^1 a^2 \cdot g(a) \, da = \int_0^1 a^2 \, da
$$

Compute the integral:

$$
\int_0^1 a^2 \, da = \left[\frac{a^3}{3}\right]_0^1 = \frac{1}{3}
$$

Now compute variance:

$$
V(A) = \frac{1}{3} - \left(\frac{1}{2}\right)^2 = \frac{1}{3} - \frac{1}{4} = \frac{4 - 3}{12} = \frac{1}{12}
$$

**Solution**

- $E(A) = \dfrac{1}{2}$
- $V(A) = \dfrac{1}{12}$

### Finding $\alpha$ and $\beta$

Let $B$ be a random variable defined as $B = X + Y$, where $X$ and $Y$ are independent exponential random variables with rate $1$:

- $X \sim \text{Exp}(1)$
- $Y \sim \text{Exp}(1)$

Determine the distribution and parameters of $B$, assuming $B \sim \text{Gamma}(\alpha, \beta)$.

**Step 1: Recognize Summation Property of Exponentials**

The sum of $n$ independent exponential random variables with rate $\lambda$ follows a gamma distribution:

$$
X_1 + X_2 + \dots + X_n \sim \text{Gamma}(\alpha = n, \beta = 1/\lambda)
$$

Since $X$ and $Y$ are independent and identically distributed $\text{Exp}(1)$, then:

$$
B = X + Y \sim \text{Gamma}(2, 1)
$$

**Step 2: Verify Using Gamma PDF and Moments**

From the gamma PDF:

$$
f(b) = \frac{b^{\alpha - 1} e^{-b/\beta}}{\beta^{\alpha} \Gamma(\alpha)}, \quad b > 0
$$

Using:

- Mean: $\mu = \alpha \beta$
- Variance: $\sigma^2 = \alpha \beta^2$

For $B$, we previously derived the marginal density:

$$
g(b) = b e^{-b}, \quad b > 0
$$

This matches the gamma PDF when:

- $\alpha = 2$
- $\beta = 1$

Check the moments:

- Mean: $\mu = 2 \cdot 1 = 2$
- Variance: $\sigma^2 = 2 \cdot 1^2 = 2$

**Solution**

$B \sim \text{Gamma}(\alpha = 2, \ \beta = 1)$

This is justified by both the summation property of exponential distributions and the match with the gamma density formula and moment equations.

### Finding $E(B)$ and $V(B)$

We will use both known properties of the Gamma Distribution and Expected values methods to find $E(B)$ and $V(B)$. Doing it both ways allows us to double check our work and ensure our logic is sound.

*Method 1: Using Known Properties of the Gamma Distribution*

The sum of $n$ independent exponential($\lambda$) random variables follows a Gamma distribution with:

- Shape parameter: $\alpha = n$
- Rate parameter: $\lambda$ (implies $\beta = 1/\lambda$ as the scale)

Since $B = X + Y$ is the sum of two $\text{Exp}(1)$ variables:

$$
B \sim \text{Gamma}(\alpha = 2, \beta = 1)
$$

Then, the mean and variance are:

- $E(B) = \alpha \beta = 2 \cdot 1 = 2$
- $V(B) = \alpha \beta^2 = 2 \cdot 1^2 = 2$

*Method 2: Directly Using Expected Value Properties*

Since expectation is linear:

$$
E(B) = E(X + Y) = E(X) + E(Y) = 1 + 1 = 2
$$

And for independent variables:

$$
V(B) = V(X + Y) = V(X) + V(Y) = 1 + 1 = 2
$$

**Solution**

- $E(B) = 2$
- $V(B) = 2$

### CDF of B

---
title: "CDF Derivation for B"
output: pdf_document
---

Let $B = X + Y$ where $X$ and $Y$ are independent $\text{Exp}(1)$ random variables. From previous results, we know that:

$$
B \sim \text{Gamma}(\alpha = 2, \ \beta = 1)
$$

We aim to derive the Cumulative Distribution Function (CDF) of $B$, denoted by $F_B(b)$.

*Step 1: Start with Gamma PDF*

The probability density function (PDF) of the gamma distribution with shape $\alpha = 2$ and rate $\lambda = 1$ (i.e., scale $\beta = 1$) is:

$$
f_B(b) = b e^{-b}, \quad b > 0
$$

*Step 2: Definition of the CDF*

The CDF is defined as:

$$
F_B(b) = \int_0^b f_B(t) \, dt = \int_0^b t e^{-t} \, dt
$$

*Step 3: Integration by Parts*

Let’s compute:

$$
\int_0^b t e^{-t} \, dt
$$

Use integration by parts:

- Let $u = t$ ⇒ $du = dt$
- Let $dv = e^{-t} dt$ ⇒ $v = -e^{-t}$

Then:

$$
\int_0^b t e^{-t} dt = -t e^{-t} \big|_0^b + \int_0^b e^{-t} dt
$$

Evaluate each term:

- First term: $-b e^{-b} + 0$
- Second term: $\left[ -e^{-t} \right]_0^b = -e^{-b} + 1$

So:

$$
F_B(b) = \int_0^b t e^{-t} dt = -b e^{-b} + \left(1 - e^{-b}\right)
$$

Simplify:

$$
F_B(b) = 1 - e^{-b}(b + 1)
$$

**Solution**

The CDF of $B$ is:

$$
F_B(b) = 1 - e^{-b}(b + 1), \quad b > 0
$$
**Plot**

Function to plot CDF is packaged in 'Lab8KORBMath4753::plot_B_cdf()'.

```{r}
plot_B_cdf()
```

*Graph Description*

The above graph illustrates the cumulative distribution function (CDF) of the random variable $B = X + Y$, where $X$ and $Y$ are independent and identically distributed exponential variables with rate $\lambda = 1$. This implies that $B \sim \text{Gamma}(2,1)$.

The horizontal axis represents values of $b$, ranging approximately from $0$ to $10$, while the vertical axis shows the cumulative probability $F_B(b)$ ranging from $0$ to $1$. The curve starts at the origin $(0, 0)$ and increases smoothly, approaching the upper bound of $1$ as $b$ grows large.

*Curve Implications*

The shape of the curve conveys several important properties of the distribution of $B$:

- **Monotonic Increase:** The CDF is strictly increasing, as expected of any cumulative distribution. It indicates the probability that $B$ takes a value less than or equal to $b$.
  
- **Early Steepness:** The curve rises steeply in the interval $b \in [0, 3]$, revealing that most of the probability mass is concentrated near smaller values of $B$. This is typical for Gamma distributions with shape parameter $\alpha = 2$.

- **Asymptotic Behavior:** As $b$ increases beyond $6$, the curve flattens and asymptotically approaches $1$, signifying diminishing additional probability mass.

- **Theoretical CDF:** The functional form for the Gamma(2,1) CDF is:
  $$
  F_B(b) = 1 - e^{-b}(b + 1)
  $$
  which aligns with the curvature observed in the graph.

*Comparative Insight*

Compared to the marginal density $g(b)$ (which peaks and then decreases), the CDF accumulates probability and smooths out the peak behavior into a continuous ascent. This curve highlights how the Gamma distribution expresses skewed, right-tailed behavior while maintaining finite expectation and variance.

This plot visually confirms theoretical properties of the Gamma(2,1) distribution and serves as a valuable reference when validating simulation-based outputs, such as those generated by functions like **mysimB()** or **rmyF()** within the package.

## Task 2: Simulation

### Simulate $A = \frac{X}{X + Y}$ 

Function is packaged in 'Lab8KORBMath4753::mysimA()'.

```{r}
mysimA(iter = 1000)
```

The histogram of $A_{\text{sim}}$ illustrates a uniform distribution across the interval $[0, 1]$. The density appears flat and constant, which aligns with the known result that if $X, Y \sim \text{Exp}(1)$, then:
$$
A = \frac{X}{X + Y} \sim \text{Uniform}(0, 1)
$$

This uniformity implies that every value within the interval has equal probability. It also results in linear cumulative growth and constant density, which is verified both analytically and via simulation.

### Simulate $B=X+Y$

```{r}
mysimB(iter = 1000)
```

In contrast to the histogram of $A_{\text{sim}}$, the histogram of $B_{\text{sim}}$ follows a right-skewed distribution. It rises quickly to a peak where $b \approx 1.7$, then decays gradually as $b$ increases. This shape is consistent with the Gamma distribution:
$$
B = X + Y \sim \text{Gamma}(2, 1)
$$
Theoretical density: $f_B(b) = b e^{-b}$

This implies:

- Most simulated values cluster at lower $b$ values.
- The density has positive skewness and a longer tail to the right.
- The variance is higher compared to $A$, contributing to broader spread.

### Simulate $B$

```{r}
rmyF(iter = 10000)
```

The function **rmyF()** simulates values of the random variable $B = X + Y$, where $X$ and $Y$ are exponential with rate 1. Instead of using built-in sampling, it numerically inverts the cumulative distribution function:
$$
F_B(b) = 1 - e^{-b}(b + 1)
$$
This method matches uniformly generated values $u \in [0, 1]$ to the corresponding $b$ values using optimization.

The output histogram shows a right-skewed, unimodal shape, consistent with the Gamma(2,1) distribution. The blue curve overlay represents the theoretical density:
$$
f_B(b) = b e^{-b}
$$

Overall, **rmyF()** accurately replicates the shape and statistical properties of the target distribution, making it a useful tool for simulation.

## Task 3: SHINY

Our SHINY app will simulate $B$ using a single controlling input for the iterations option in the above function.

![](images/SHINYApp_Lab8.png)
